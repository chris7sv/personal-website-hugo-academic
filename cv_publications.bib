@article{bowersDeepProblemsNeural2022,
  title = {Deep {{Problems}} with {{Neural Network Models}} of {{Human Vision}}},
  author = {Bowers, Jeffrey S. and Malhotra, Gaurav and Dujmovi{\'c}, Marin and Montero, Milton Llera and Tsvetkov, Christian and Biscione, Valerio and Puebla, Guillermo and Adolfi, Federico and Hummel, John E. and Heaton, Rachel F. and Evans, Benjamin D. and Mitchell, Jeffrey and Blything, Ryan},
  year = {2022},
  month = dec,
  journal = {Behavioral and Brain Sciences},
  pages = {1--74},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X22002813},
  abstract = {Deep neural networks (DNNs) have had extraordinary successes in classifying photographic images of objects and are often described as the best models of biological vision. This conclusion is largely based on three sets of findings: (1) DNNs are more accurate than any other model in classifying images taken from various datasets, (2) DNNs do the best job in predicting the pattern of human errors in classifying objects taken from various behavioral datasets, and (3) DNNs do the best job in predicting brain signals in response to images taken from various brain datasets (e.g., single cell responses or fMRI data). However, these behavioral and brain datasets do not test hypotheses regarding what features are contributing to good predictions and we show that the predictions may be mediated by DNNs that share little overlap with biological vision. More problematically, we show that DNNs account for almost no results from psychological research. This contradicts the common claim that DNNs are good, let alone the best, models of human object recognition. We argue that theorists interested in developing biologically plausible models of human vision need to direct their attention to explaining psychological findings. More generally, theorists need to build models that explain the results of experiments that manipulate independent variables designed to test hypotheses rather than compete on making the best predictions. We conclude by briefly summarizing various promising modelling approaches that focus on psychological data.},
  langid = {english},
  keywords = {Brain-Score,Computational Neuroscience,Deep Neural Networks,Human Vision,Object recognition},
  file = {/home/ctsv/Zotero/storage/Q9PBPHPT/Bowers et al. - 2022 - Deep Problems with Neural Network Models of Human .pdf}
}

@article{Tsvetkov2022.03.31.486580,
  title = {The Role of Capacity Constraints in {{Convolutional Neural Networks}} for Learning Random versus Natural Data},
  author = {Tsvetkov, Christian and Malhotra, Gaurav and Evans, Benjamin D. and Bowers, Jeffrey S.},
  year = {2022},
  journal = {bioRxiv : the preprint server for biology},
  eprint = {https://www.biorxiv.org/content/early/2022/12/26/2022.03.31.486580.full.pdf},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2022.03.31.486580},
  abstract = {Convolutional neural networks (CNNs) are often described as promising models of human vision, yet they show many differences from human abilities. We focus on a superhuman capacity of top-performing CNNs, namely, their ability to learn very large datasets of random patterns. We verify that human learning on such tasks is extremely limited, even with few stimuli. We argue that the performance difference is due to CNNs' overcapacity and introduce biologically inspired mechanisms to constrain it, while retaining the good test set generalisation to structured images as characteristic of CNNs. We investigate the efficacy of adding noise to hidden units' activations, restricting early convolutional layers with a bottleneck, and using a bounded activation function. Internal noise was the most potent intervention and the only one which, by itself, could reduce random data performance in the tested models to chance levels. We also investigated whether networks with biologically inspired capacity constraints show improved generalisation to out-of-distribution stimuli, however little benefit was observed. Our results suggest that constraining networks with biologically motivated mechanisms paves the way for closer correspondence between network and human performance, but the few manipulations we have tested are only a small step towards that goal.Competing Interest StatementThe authors have declared no competing interest.},
  elocation-id = {2022.03.31.486580}
}

@inproceedings{tsvetkovAddingBiologicalConstraints2020,
  title = {Adding Biological Constraints to Deep Neural Networks Reduces Their Capacity to Learn Unstructured Data},
  booktitle = {Proceedings of the 42nd {{Annual Conference}} of the {{Cognitive Science Society}} 2020},
  author = {Tsvetkov, Christian and Malhotra, Gaurav and Evans, Benjamin D and Bowers, Jeffrey S},
  year = {2020},
  volume = {42},
  pages = {2358--2364},
  langid = {english},
  file = {/home/ctsv/Zotero/storage/2ND4ZKMR/Tsvetkov et al. - Adding biological constraints to deep neural netwo.pdf}
}

@mastersthesis{tsvetkovHowDeepNeural2018,
  title = {How {{Do Deep Neural Networks Represent Faces}}?},
  author = {Tsvetkov, Christian},
  year = {2018},
  address = {{Sofia, Bulgaria}},
  school = {New Bulgarian University}
}
